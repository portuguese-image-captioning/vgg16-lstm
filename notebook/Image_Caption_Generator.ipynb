{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2tRC9dVKmpk"
   },
   "source": [
    "# Image Caption Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBesXDKyKmpm"
   },
   "source": [
    "### Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Pickle\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "#Keras\n",
    "#Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#NTLK\n",
    "import nltk\n",
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#corpus-bleu evaluation metrics\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.layers import Input\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoL5xSi0Kmpo"
   },
   "source": [
    "### Preprocess image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBipZAkD4ICJ",
    "tags": []
   },
   "source": [
    "### extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "g8WGMQ8nKmpp"
   },
   "outputs": [],
   "source": [
    "# extrat Features\n",
    "def extract_features(directory):\n",
    "    \n",
    "    model = VGG16( include_top=False,input_tensor=Input(shape=(128, 128, 3)))\n",
    "    \n",
    "    #remove last layer\n",
    "    #model.layers.pop()\n",
    "    model = Model(inputs = model.inputs , outputs = model.layers[-1].output)\n",
    "    \n",
    "    #print(model.summary())\n",
    "    \n",
    "    features = dict()\n",
    "    \n",
    "    #for name in listdir(directory):\n",
    "    dirs  = listdir(directory)\n",
    "    count = 0\n",
    "    for name in tqdm(dirs, desc='dirs'):\n",
    "        pass\n",
    "        \n",
    "        # load and image\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename , target_size=(128 , 128))\n",
    "        \n",
    "        image = img_to_array(image)\n",
    "        \n",
    "        image = image.reshape((1 , image.shape[0] , image.shape[1] ,image.shape[2]))\n",
    "        \n",
    "        image = preprocess_input(image)\n",
    "        \n",
    "        \n",
    "        feature = model.predict(image , verbose = 0)\n",
    "        \n",
    "        # get image id\n",
    "        image_id = name.split(\".\")[0]\n",
    "        \n",
    "        # store features\n",
    "        features[image_id] = feature\n",
    "        \n",
    "        #print(name)\n",
    "        count = count + 1\n",
    "        if count == 10000:\n",
    "            break\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVsQbSF6Kmpp",
    "outputId": "a90f5428-9079-4de3-e9fa-4c7173eec684"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dirs:   0%|                                                                                                                                                                            | 0/118287 [00:00<?, ?it/s]<timed exec>:39: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "dirs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 118287/118287 [4:50:19<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4h 3min 54s\n",
      "Wall time: 4h 52min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train\n",
    "#directory = '/content/val2017'\n",
    "#Valid Path\n",
    "directory = 'D:/SiDi/Project/Modulo III/dataset/train2017'\n",
    "#directory = 'D:/SiDi/Project/Modulo III/dataset/val2017'\n",
    "\n",
    "#features = extract_features(directory)\n",
    "#print('extracted features :',len(features))\n",
    "\n",
    "model = VGG16()\n",
    "#model = VGG16( include_top=False,input_tensor=Input(shape=(224, 224, 3)))\n",
    "    \n",
    "#remove last layer\n",
    "#model.layers.pop()\n",
    "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "features = dict()\n",
    "\n",
    "#for name in listdir(directory):\n",
    "dirs  = listdir(directory)\n",
    "#Train Image Dataset Path\n",
    "#dirs  = listdir(TRAIN_IMAGE_DATASET_PATH)\n",
    "#dirs = sorted(dirs)[0:10000]\n",
    "#dirs = sorted(dirs)\n",
    "count = 0\n",
    "for name in tqdm(dirs, desc='dirs'):\n",
    "    #pass\n",
    "\n",
    "    # load and image\n",
    "    filename = directory + '/' + name\n",
    "    #image = load_img(filename , target_size=(128 , 128))\n",
    "    image = load_img(filename , target_size=(224 , 224))\n",
    "    #print(type(image))\n",
    "    #target_size=(128 , 128)\n",
    "    image = load_img(filename)\n",
    "\n",
    "    image = image.resize((224, 224), Image.ANTIALIAS)\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    image = image.reshape((1 , image.shape[0] , image.shape[1] ,image.shape[2]))\n",
    "\n",
    "    image = preprocess_input(image)\n",
    "\n",
    "\n",
    "    feature = model.predict(image , verbose = 0)\n",
    "\n",
    "    # get image id\n",
    "    image_id = name.split(\".\")[0]\n",
    "\n",
    "    # store features\n",
    "    features[image_id] = feature\n",
    "    #print(image_id)\n",
    "    #print(name)\n",
    "    \n",
    "#dump(features , open('./val_features.pkl' , 'wb'))\n",
    "dump(features , open('./train_features.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(features , open('./train_features.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NUZVqduKmpq"
   },
   "source": [
    "### Create Vocabulary from Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(features , open('features.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qveztP424M6y"
   },
   "source": [
    "### Load Decriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ExArJ80XKmpq"
   },
   "outputs": [],
   "source": [
    "#lemma = WordNetLemmatizer()\n",
    "\n",
    "def load_decriptions(doc):\n",
    "    mapping = dict()\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        #image_id = doc['image_id'][i]\n",
    "        image_id = doc['filename'][i]\n",
    "        image_desc = doc['desc'][i]\n",
    "        \n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        \n",
    "        mapping[image_id].append(image_desc)\n",
    "        \n",
    "    return mapping\n",
    "\n",
    "def clean_text(desc):\n",
    "    \n",
    "    # clean punctuation\n",
    "    desc = re.sub(r'[^\\w\\s]' ,'', desc)\n",
    "    \n",
    "    # tokenize the words\n",
    "    desc = desc.split()\n",
    "    \n",
    "    # convert to lower case\n",
    "    desc = [token.lower() for token in desc]\n",
    "    \n",
    "    # lemmatization\n",
    "    #desc = [lemma.lemmatize(token) for token in desc]\n",
    "    \n",
    "    # remove numerical values\n",
    "    desc = [token for token in desc if token.isalpha()]\n",
    "    \n",
    "    # join whole token\n",
    "    desc = ' '.join(desc)\n",
    "    \n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Y54rkGbiKmpr"
   },
   "outputs": [],
   "source": [
    "# convert loaded descriptions into vocablury\n",
    "def to_vocabluary(descriptions):\n",
    "    all_desc = set()\n",
    "    \n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc\n",
    "\n",
    "def save_descriptions(descriptions , filename):\n",
    "    lines = list()\n",
    "    \n",
    "    for key , desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key +' '+ desc)\n",
    "            \n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename , 'w')\n",
    "    file.write(data)\n",
    "    file.close()              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sZwvrBekiW2"
   },
   "source": [
    "### Load Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rdx7ThYLKmps"
   },
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv('DataframeFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "-fUuJoTzKmps",
    "outputId": "a9c0ebeb-ca19-464c-8816-c08cc8af7696"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>caption</th>\n",
       "      <th>filename</th>\n",
       "      <th>rotulos</th>\n",
       "      <th>image_height</th>\n",
       "      <th>image_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Esta com o braco esquerdo erguido, segurando ...</td>\n",
       "      <td>i-00000001.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>960</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>Ao centro uma arvore de natal com decoracao d...</td>\n",
       "      <td>i-00000002.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>608</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>A sexta pessoa usa uma blusa listrada em pret...</td>\n",
       "      <td>i-00000002.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>608</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>Ao centro, um cano grosso na vertical e um fi...</td>\n",
       "      <td>i-00000004.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1081</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Entre os canos e a parede cresce uma planta d...</td>\n",
       "      <td>i-00000004.jpg</td>\n",
       "      <td>train</td>\n",
       "      <td>1081</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            caption  \\\n",
       "0           4   Esta com o braco esquerdo erguido, segurando ...   \n",
       "1          11   Ao centro uma arvore de natal com decoracao d...   \n",
       "2          14   A sexta pessoa usa uma blusa listrada em pret...   \n",
       "3          19   Ao centro, um cano grosso na vertical e um fi...   \n",
       "4          20   Entre os canos e a parede cresce uma planta d...   \n",
       "\n",
       "         filename rotulos  image_height  image_width  \n",
       "0  i-00000001.jpg   train           960          960  \n",
       "1  i-00000002.jpg   train           608         1080  \n",
       "2  i-00000002.jpg   train           608         1080  \n",
       "3  i-00000004.jpg   train          1081         1080  \n",
       "4  i-00000004.jpg   train          1081         1080  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB0sahnVyh42",
    "outputId": "fe1f7267-143f-4eeb-d459-faf442eccfe5"
   },
   "outputs": [],
   "source": [
    "nltk.download('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dHG01rqUKmpt"
   },
   "outputs": [],
   "source": [
    "descriptions['desc'] = descriptions['caption'].apply(lambda x : clean_text(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RMcSX6HKmpt",
    "outputId": "4d2fbefc-1159-4380-94b8-6d99e25713e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(descriptions['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q9Mr0xzD8LF8",
    "outputId": "74c439c3-192b-4972-def0-fc9ed80d5c59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'esta com o braco esquerdo erguido segurando um crucifixo preso por uma alca'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions['desc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "M4vwKYBAhy2H",
    "outputId": "c2fe6459-652d-4349-a869-51e9d7e70603"
   },
   "outputs": [],
   "source": [
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcXz7EbAk5Jl"
   },
   "source": [
    "### Remove Outlier Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUDjjDG0iHzJ"
   },
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv('results.csv' , sep=',')\n",
    "descriptions.columns = ['image_id','no','desc']\n",
    "#descriptions = descriptions.drop(descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index, inplace=False).copy()\n",
    "#descriptions = descriptions[descriptions[\"image_id\"] != \"2258277193_586949ec62.jpg.1\"].copy()\n",
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index\n",
    "#print(f\"ANTES = {descriptions.shape}\")\n",
    "descriptions = descriptions.drop(descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index, axis=0, inplace=False).copy()\n",
    "#print(f\"DEPOIS {descriptions.shape}\")\n",
    "\n",
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index\n",
    "#descriptions.iloc[[6729, 6730, 6731, 6732, 6733]]\n",
    "\n",
    "descriptions.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQEQr-myusMy"
   },
   "outputs": [],
   "source": [
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index\n",
    "#descriptions.iloc[[6729, 6730, 6731, 6732, 6733]]\n",
    "\n",
    "descriptions.reset_index(inplace=True)\n",
    "#descriptions.iloc[[6729, 6730, 6731, 6732, 6733]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "bQgC9BpdKmpt",
    "outputId": "48d0ec7b-3d69-4746-b9e7-fa26dc10fe6f"
   },
   "outputs": [],
   "source": [
    "desc_map = load_decriptions(descriptions) # image name with Its captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YvgiKdTbKmpu"
   },
   "outputs": [],
   "source": [
    "vocabulary = to_vocabluary(desc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f214a89oKmpu",
    "outputId": "963d61a8-85f4-4a8a-fad3-02a6c5f7d922"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58595"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "GUUI6QNFKmpv"
   },
   "outputs": [],
   "source": [
    "save_descriptions(desc_map , 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGx2Hsy9Kmpv"
   },
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66eZpWLE4XWT"
   },
   "source": [
    "### Load Documento \n",
    "### Load Clean Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "wHyvy-07Kmpw"
   },
   "outputs": [],
   "source": [
    "# loading the doc\n",
    "def load_doc(filename):\n",
    "    file = open(filename , 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# this function is used for to get train image description from our dataset\n",
    "\n",
    "def load_clean_descriptions(filename , dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    \n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id , image_desc = tokens[0] , tokens[1:]\n",
    "        #print(f\"image_id = {image_id}\" )\n",
    "        if image_id in dataset:\n",
    "            \n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            \n",
    "            # we add two tage at start and at end of the descitpion to identify to start and \n",
    "            # end of desc.\n",
    "            desc = 'startseq '+ ' '.join(image_desc)+ ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "            \n",
    "    return descriptions\n",
    "\n",
    "\n",
    "# laod photo features\n",
    "def load_photo_features(filename , dataset):\n",
    "    all_features = load(open(filename,'rb'))\n",
    "    #print(all_features)\n",
    "    #features = {k+'.jpg' : all_features[k] for k in dataset}\n",
    "    features = {k: all_features[k.split(\".\")[0] ] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "485k7XW6Kmpw",
    "outputId": "93db8b60-6179-4fa5-c79e-9b385cb603a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train image 5162\n"
     ]
    }
   ],
   "source": [
    "#train = set(descriptions['image_id'][:127125])\n",
    "train = set(descriptions['filename'][:7999])\n",
    "print('len of train image',len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r1SeooF4Kmpx"
   },
   "outputs": [],
   "source": [
    "# traininset makinng and set startseq and endseq tag in descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt' , train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUg3F5PDKmpx",
    "outputId": "9d6b8589-198e-4813-95db-9334bef07076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of train descriptions 5162\n"
     ]
    }
   ],
   "source": [
    "print('len of train descriptions' , len(train_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8tWIJj6_Kmpx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train2 = train[0].apply(lambda x : x.replace(\\'.jpg.1\\' , \\'.jpg\\').replace(\\'.jpg\\' , \\'\\')) # remove jpg sign\\n#train2 = train2.apply(lambda x : x.replace(\\'.jpg\\' , \\'\\')) # remove jpg sign\\ntrain2 = train2.drop(train2[train2 == \"2258277193_586949ec62\"].index)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(train)\n",
    "\"\"\"train2 = train[0].apply(lambda x : x.replace('.jpg.1' , '.jpg').replace('.jpg' , '')) # remove jpg sign\n",
    "#train2 = train2.apply(lambda x : x.replace('.jpg' , '')) # remove jpg sign\n",
    "train2 = train2.drop(train2[train2 == \"2258277193_586949ec62\"].index)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "v288LnLDKmpy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       i-00002015.jpg\n",
       "1       i-00006457.jpg\n",
       "2       i-00001626.jpg\n",
       "3       i-00006132.jpg\n",
       "4       i-00007167.jpg\n",
       "             ...      \n",
       "5157    i-00007264.jpg\n",
       "5158    i-00007097.jpg\n",
       "5159    i-00003544.jpg\n",
       "5160    i-00002344.jpg\n",
       "5161    i-00002391.jpg\n",
       "Name: 0, Length: 5162, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XvrN6h8IRoy",
    "outputId": "1b39e54e-0829-4409-ed60-08fb75e625f8"
   },
   "outputs": [],
   "source": [
    "train2[train2 == \"2258277193_586949ec62\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_train = train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj3GLV1MKf7D",
    "outputId": "44650d82-c8ba-4675-cec0-fce60b403741"
   },
   "outputs": [],
   "source": [
    "len(train[0]), len(train2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = load(open('features.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Per99RcGKmpy",
    "outputId": "1174ecfa-bd1a-4287-b25f-3cd7bba19635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photos train : 5162\n"
     ]
    }
   ],
   "source": [
    "# train features means feautures from training images\n",
    "#train_features = load_photo_features('features.pkl' , train2)\n",
    "train_features = load_photo_features('features.pkl' , filename_train)\n",
    "print('photos train :',len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ucHoBNy8Kmpy"
   },
   "outputs": [],
   "source": [
    "# convert dictonary to lis descriptions\n",
    "\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "        \n",
    "    return all_desc\n",
    "\n",
    "# fit tokenizer on descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lqPhTJXKmpz",
    "outputId": "da8ece28-824d-4bf8-fb26-a81ef19d7c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 7838\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('vocab size' , vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Q3Ka8rFlKmpz"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "nwdKz6CsKmpz"
   },
   "outputs": [],
   "source": [
    "# calculate the length with most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max([len(line.split())for line in lines])\n",
    "    \n",
    "# create sequences of images,input sequences and output sequences\n",
    "def create_sequences(tokenizer , max_length , desc_list , photo):\n",
    "    X1 , X2 , y = list() , list() , list()\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        # convert words to number value\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        \n",
    "        for i in range(1, len(seq)):\n",
    "            \n",
    "            in_seq , output_seq = seq[:i] , seq[i]\n",
    "            in_seq = pad_sequences([in_seq] , maxlen = max_length)[0]\n",
    "            output_seq = to_categorical([output_seq] , num_classes = vocab_size)[0]\n",
    "            #print(photo)\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(output_seq)\n",
    "            \n",
    "    return np.asarray(X1) , np.asarray(X2) , np.asarray(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cBvI_JSKmp0"
   },
   "source": [
    "### VGG16 + LSTM DeepModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "ExTqo4aPKmp0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout\n",
    "#from tensorflow.keras.layers.merge import add\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "p13GFSYjKmp0"
   },
   "outputs": [],
   "source": [
    "def define_Model(vocab_size , max_length):\n",
    "    \n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096, ))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(512 , activation='relu')(fe1)\n",
    "    fe3 = Dense(256 , activation = 'relu')(fe2)\n",
    "    \n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size,512,mask_zero=True )(inputs2) # mask_zero = ignore padding\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(512 , return_sequences=True)(se2)\n",
    "    se4 = Dropout(0.5)(se3)\n",
    "    se5 = LSTM(256)(se4)\n",
    "    \n",
    "    \n",
    "    #decoder Model\n",
    "    decoder1 = Add()([fe3,se5])\n",
    "    decoder2 = Dense(256 , activation='relu')(decoder1)\n",
    "    decoder3 = Dense(512 , activation='relu')(decoder2)\n",
    "    outputs = Dense(vocab_size , activation='softmax')(decoder3)\n",
    "    \n",
    "    # combine both image and text\n",
    "    model = Model([inputs1 , inputs2] , outputs)\n",
    "    model.compile(loss='categorical_crossentropy' , optimizer = 'adam')\n",
    "    \n",
    "    # summary\n",
    "    #print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X4BHPFZKmp0"
   },
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "z46x5UeHKmp1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Generator ...\n",
      "Photo Array Size = (1, 4, 4, 512)\n",
      "Image Input = (14, 512)\n",
      "Seq Input = (14, 64)\n",
      "Concate Input = 2\n",
      "Photo Array Size = (1, 4, 4, 512)\n",
      "Image Input = (24, 512)\n",
      "Seq Input = (24, 64)\n",
      "Concate Input = 2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_92/dense_462/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Temp\\ipykernel_33908\\696867669.py\", line 43, in <cell line: 40>\n      model.fit(generator , epochs = 1 , steps_per_epoch = steps , verbose = 1)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 221, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'model_92/dense_462/MatMul'\nMatrix size-incompatible: In[0]: [14,512], In[1]: [4096,512]\n\t [[{{node model_92/dense_462/MatMul}}]] [Op:__inference_train_function_1530796]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Input \u001b[1;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     41\u001b[0m     generator \u001b[38;5;241m=\u001b[39m data_generator(train_descriptions , train_features , tokenizer , max_len)\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#model.fit()\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_92/dense_462/MatMul' defined at (most recent call last):\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\traitlets\\config\\application.py\", line 972, in launch_instance\n      app.start()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n      result = self._run_cell(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n      return runner(coro)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\rbsa2\\AppData\\Local\\Temp\\ipykernel_33908\\696867669.py\", line 43, in <cell line: 40>\n      model.fit(generator , epochs = 1 , steps_per_epoch = steps , verbose = 1)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1409, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function\n      return step_function(self, iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n      y_pred = self(x, training=True)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 490, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\functional.py\", line 458, in call\n      return self._run_internal_graph(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\functional.py\", line 596, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1014, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\layers\\core\\dense.py\", line 221, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'model_92/dense_462/MatMul'\nMatrix size-incompatible: In[0]: [14,512], In[1]: [4096,512]\n\t [[{{node model_92/dense_462/MatMul}}]] [Op:__inference_train_function_1530796]"
     ]
    }
   ],
   "source": [
    "# below code progressivly load the code in batches\n",
    "def data_generator(descriptions , photos , tokenizer , max_length):\n",
    "    while 1:\n",
    "        print(\"Data Generator ...\")\n",
    "        for key , desc_list in descriptions.items():\n",
    "            print(f\"Photo Array Size = {photos[key].shape}\")\n",
    "            #print(photos[key])\n",
    "            #print(photos[key][0])\n",
    "            photo = photos[key][0][0][0]\n",
    "            \"\"\"photos3d = photos[key][0]\n",
    "            temp = []\n",
    "            for photos2d in photos3d:\n",
    "                for photo1d in photos2d:\n",
    "                    #print(f\"Photo = {photo1d.shape}\")\n",
    "                    #break\n",
    "                    temp.append(photo1d)\n",
    "            photo = np.asarray(temp)\n",
    "            print(f\"Photo = {photo.shape}\")\n",
    "            #print(f\"Temp = {len(temp)}\")\n",
    "            \"\"\"\n",
    "            in_img , in_seq , out_seq = create_sequences(tokenizer , max_length , desc_list , photo)\n",
    "            print(f\"Image Input = {in_img.shape}\")\n",
    "            print(f\"Seq Input = {in_seq.shape}\")\n",
    "            print(f\"Concate Input = {len([in_img , in_seq])}\")\n",
    "            \"\"\"temp = [] \n",
    "            for i in range(0,in_img.shape[0]):\n",
    "                temp.append(in_img[i][0][0])\n",
    "            in_img = np.asarray(temp)\"\"\"\n",
    "            #yield in_img , in_seq, out_seq\n",
    "            #yield[[in_img , in_seq] , out_seq]\n",
    "                    \n",
    "            \"\"\"in_img = in_img.reshape(in_img.shape[1],in_img.shape[0])\"\"\"\n",
    "            yield [[ in_img , in_seq], out_seq]\n",
    "            \n",
    "            \n",
    "model = define_Model(vocab_size , max_len)\n",
    "epochs = 5\n",
    "steps = len(train_descriptions)\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions , train_features , tokenizer , max_len)\n",
    "    \n",
    "    model.fit(generator , epochs = 1 , steps_per_epoch = steps , verbose = 1)\n",
    "    #model.fit()\n",
    "    \n",
    "    model.save('model_'+ str(i+1) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UWppXY55Ui4"
   },
   "source": [
    "<b> Vocabulary Size Info  </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dugtQwpFKmp1",
    "outputId": "a7a5acf5-11de-4e98-d929-0e42dc0734ec"
   },
   "outputs": [],
   "source": [
    "print('len :', len(train))\n",
    "print('Descriptions :',len(train_descriptions))\n",
    "print('photos train :',len(train_features))\n",
    "print('Vocabulary size :',vocab_size)\n",
    "max_len = max_length(train_descriptions)\n",
    "print('Description max length :', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDQfi5CWKmp1"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgYW74OgKmp2"
   },
   "source": [
    "<b> Training Process </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DPf_upSN6gU",
    "outputId": "fa38c0c1-efde-4165-cdce-5a970a209b2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7838, 64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size,max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sar1zQYQKmp2",
    "outputId": "1b2504cf-416f-447e-d31b-aa4ca64580fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_27 (InputLayer)          [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 64, 512)      4013056     ['input_27[0][0]']               \n",
      "                                                                                                  \n",
      " input_26 (InputLayer)          [(None, 4096)]       0           []                               \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 64, 512)      0           ['embedding_12[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 4096)         0           ['input_26[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_24 (LSTM)                 (None, 64, 512)      2099200     ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 512)          2097664     ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 64, 512)      0           ['lstm_24[0][0]']                \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 256)          131328      ['dense_51[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_25 (LSTM)                 (None, 256)          787456      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 256)          0           ['dense_52[0][0]',               \n",
      "                                                                  'lstm_25[0][0]']                \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 256)          65792       ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_54 (Dense)               (None, 512)          131584      ['dense_53[0][0]']               \n",
      "                                                                                                  \n",
      " dense_55 (Dense)               (None, 7838)         4020894     ['dense_54[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,346,974\n",
      "Trainable params: 13,346,974\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Data Generator ...\n",
      "Photo Size = (4, 4, 512)\n",
      "Image Input = (14, 4, 4, 512)\n",
      "Seq Input = (14, 64)\n",
      "Image Input = (14, 4, 4, 512)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 4096) for input KerasTensor(type_spec=TensorSpec(shape=(None, 4096), dtype=tf.float32, name='input_26'), name='input_26', description=\"created by layer 'input_26'\"), but it was called on an input with incompatible shape (None, None, None, None).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, None, None, 7838) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      6\u001b[0m     generator \u001b[38;5;241m=\u001b[39m data_generator(train_descriptions , train_features , tokenizer , max_len)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#model.fit()\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileetnw1t5s.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\losses.py\", line 1787, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"D:\\SiDi\\Project\\Modulo III\\experiment\\image-caption-generator\\lib\\site-packages\\keras\\backend.py\", line 5119, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, None) and (None, None, None, 7838) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model = define_Model(vocab_size , max_len)\n",
    "epochs = 5\n",
    "steps = len(train_descriptions)\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions , train_features , tokenizer , max_len)\n",
    "    \n",
    "    model.fit(generator , epochs = 1 , steps_per_epoch = steps , verbose = 1)\n",
    "    #model.fit()\n",
    "    \n",
    "    model.save('model_'+ str(i+1) + '.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOTTlaR9Kmp2"
   },
   "source": [
    "### Bleu Score For Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zC7DEQyKmp2"
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer , tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezaZUr6gKmp2"
   },
   "outputs": [],
   "source": [
    "def generate_desc(model , tokenizer , photo , max_length):\n",
    "    \n",
    "    input_text = 'startseq'\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        \n",
    "        sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        sequence = pad_sequences([sequence] , maxlen=max_length)\n",
    "        \n",
    "        # predict the next word\n",
    "        next_word_id = model.predict([photo,sequence],verbose = 0)\n",
    "        \n",
    "        # get highest probality word from list of words\n",
    "        next_word_id = np.argmax(next_word_id)\n",
    "        \n",
    "        # get word from id\n",
    "        word = word_for_id(next_word_id , tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        # update input text\n",
    "        input_text += ' '+ word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2dC85xPKmp3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUTc2zNKKmp3"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model , desciptions , photos , tokenizer , max_length):\n",
    "    actual , predicted = list() , list()\n",
    "    \n",
    "    for key , desc_list in desciptions.items():\n",
    "        generated_desc = generate_desc(model , tokenizer , photos[key] , max_length)\n",
    "        \n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(generated_desc.split())\n",
    "    \n",
    "    print('Bleu_Score -1 = %f'%corpus_bleu(actual , predicted , weights=(1,0,0,0)))\n",
    "    \n",
    "    print('Bleu_Score -2 = %f'%corpus_bleu(actual , predicted , weights=(0.5,0.5,0,0)))\n",
    "    \n",
    "    print('Bleu_Score -3 = %f'%corpus_bleu(actual , predicted , weights=(0.33,0.33,0.33,0))) \n",
    "    \n",
    "    print('Bleu_Score -4 = %f'%corpus_bleu(actual , predicted , weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIuJU3V9Kmp3",
    "outputId": "b0136cb0-f550-421a-afba-c89e84f1ade1"
   },
   "outputs": [],
   "source": [
    "127125+(5000*5) # 5000 images for testing each image has 5 captions so that is calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usgOXrmhKmp4",
    "outputId": "87461e4b-62de-49c1-c2e1-6420f19785d7"
   },
   "outputs": [],
   "source": [
    "158915-152125 # reaminng Images total 158915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdASiDkDKmp4"
   },
   "source": [
    "2nd test Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCTa17iJKmp4",
    "outputId": "41594e0d-77ff-4d8f-cb6d-0b9f357f046f"
   },
   "outputs": [],
   "source": [
    "6790 / 5 # test Set Which test at the last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8rts4COKmp4"
   },
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xw8RXnBJgVBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbjjzsfeEm21",
    "outputId": "c1b8d635-75fe-4d80-b4a8-3494ecb8d2e9"
   },
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv('results.csv' , sep=',')\n",
    "descriptions.columns = ['image_id','no','desc']\n",
    "#descriptions = descriptions.drop(descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index, inplace=False).copy()\n",
    "#descriptions = descriptions[descriptions[\"image_id\"] != \"2258277193_586949ec62.jpg.1\"].copy()\n",
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index\n",
    "#print(f\"ANTES = {descriptions.shape}\")\n",
    "descriptions = descriptions.drop(descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index, axis=0, inplace=False).copy()\n",
    "#print(f\"DEPOIS {descriptions.shape}\")\n",
    "\n",
    "descriptions[descriptions[\"image_id\"] == \"2258277193_586949ec62.jpg.1\"].index\n",
    "#descriptions.iloc[[6729, 6730, 6731, 6732, 6733]]\n",
    "\n",
    "descriptions.reset_index(inplace=True)\n",
    "test = set(descriptions['image_id'][20227:])\n",
    "print('len of train image',len(test))\n",
    "\n",
    "#test2 = test[0].apply(lambda x : x.replace('.jpg' , '')) # remove jpg sign\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt',test)\n",
    "print('len of train image',len(test_descriptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocaApG4BKc3f"
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(test)\n",
    "test2 = test[0].apply(lambda x : x.replace('.jpg.1' , '.jpg').replace('.jpg' , '')) # remove jpg sign\n",
    "#train2 = train2.apply(lambda x : x.replace('.jpg' , '')) # remove jpg sign\n",
    "test2 = test2.drop(test2[test2 == \"2258277193_586949ec62\"].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUmz_xRcKmp5"
   },
   "outputs": [],
   "source": [
    "test_features = load_photo_features('features.pkl',test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ey1vN_5UKmp5"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BViFlBZLKmp5"
   },
   "outputs": [],
   "source": [
    "filename = \"/content/model_5.h5\"\n",
    "model = load_model(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0vN0sH8Kmp5",
    "outputId": "5fd70ef9-b81f-4c4e-831b-daa95b3e54e5"
   },
   "outputs": [],
   "source": [
    "print('len of test dataset',len(test))\n",
    "print('len of Descriptions',len(test_descriptions))\n",
    "print('len of Test features',len(test_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nDmAZNO11FV"
   },
   "source": [
    "### Evaluate Model Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcpPIhauKmp6"
   },
   "outputs": [],
   "source": [
    "evaluate_model(model , test_descriptions , test_features , tokenizer , max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnckJwbkKmp6"
   },
   "outputs": [],
   "source": [
    "dump(tokenizer , open('tokenizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTxNAmKFKmp6"
   },
   "source": [
    "### Testing on Individual Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMeu38IHKmp6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lY6sc4_WKmp7"
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer , tokenizer):\n",
    "    for word,index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "        \n",
    "def generate_desc(model , tokenizer , photo , max_length):\n",
    "    \n",
    "    input_text = 'startseq'\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        \n",
    "        sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        sequence = pad_sequences([sequence] , maxlen=max_length)\n",
    "        \n",
    "        # predict the next word\n",
    "        next_word_id = model.predict([photo,sequence],verbose = 0)\n",
    "        \n",
    "        # get highest probality word from list of words\n",
    "        next_word_id = np.argmax(next_word_id)\n",
    "        \n",
    "        # get word from id\n",
    "        word = word_for_id(next_word_id , tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        # update input text\n",
    "        input_text += ' '+ word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGhnwAD6Kmp7"
   },
   "outputs": [],
   "source": [
    "def extract_features_for_one_image(filename):\n",
    "    model = VGG16()\n",
    "    model.layers.pop()\n",
    "    \n",
    "    model = Model(inputs = model.inputs , outputs = model.layers[-1].output)\n",
    "    \n",
    "    image = load_img(filename , target_size=(224,224))\n",
    "    \n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    image = image.reshape((1,image.shape[0],image.shape[1],image.shape[2]))\n",
    "    \n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    feature = model.predict(image , verbose = 0)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkZ6n7twKmp7"
   },
   "outputs": [],
   "source": [
    "def get_image_caption(filename,model):\n",
    "    tokenizer = load(open('tokenizer.pkl','rb'))\n",
    "    photo = extract_features_for_one_image(filename)\n",
    "    desc = generate_desc(model , tokenizer , photo , 80)\n",
    "    desc = desc.replace('startseq','')\n",
    "    desc = desc.replace('endseq','')\n",
    "    return desc.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "wCy8OLSCKmp7",
    "outputId": "e82616e9-6d38-4487-b527-19448216621e"
   },
   "outputs": [],
   "source": [
    "model2 = load_model('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXKzW-YbKmp7"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image,display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTFFm9lyKmp8"
   },
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LA7jAxMGKmp8"
   },
   "outputs": [],
   "source": [
    "filename = 'test_image.jpg'\n",
    "print(get_image_caption(filename,model2))\n",
    "display(Image(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvbmiJUKKmp8"
   },
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUQXkp_kKmp8"
   },
   "outputs": [],
   "source": [
    "filename = 'test_image2.jpg'\n",
    "print(get_image_caption(filename,model2))\n",
    "display(Image(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxaWcIJJKmp8"
   },
   "outputs": [],
   "source": [
    "filename = '984950.jpg'\n",
    "print(get_image_caption(filename,model2))\n",
    "display(Image(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FrnP9q-hKmp9"
   },
   "outputs": [],
   "source": [
    "filename = 'MyImage.jpg'\n",
    "print(get_image_caption(filename,model2))\n",
    "display(Image(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f63hitW1Kmp9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cópia de Image Caption Generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "image-caption-generator",
   "language": "python",
   "name": "image-caption-generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
